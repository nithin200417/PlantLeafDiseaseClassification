{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":182633,"sourceType":"datasetVersion","datasetId":78313},{"sourceId":7750964,"sourceType":"datasetVersion","datasetId":4531691}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\n\npath='/kaggle/input/plantvillage29classes/train/'\nplt.figure(figsize=(70,70))\ncount=0\nplant_names=[]\ntotal_images=0\n\nfor i in os.listdir(path):\n  count+=1\n  plant_names.append(i)\n\n  images_path=[img for img in os.listdir(path+\"/\"+i) if img.endswith(('.png', '.jpg', '.jpeg'))]\n  print(\"Number of images of \"+i+\":\",len(images_path),\"||\",end=\" \")\n  total_images+=len(images_path)\n\n  if len(images_path) > 0:\n    image_show=plt.imread(path+\"/\"+i+\"/\"+images_path[0])\n    #plt.imshow(image_show)\n    #plt.xlabel(i)\n  \n  #plt.xticks([])\n  #plt.yticks([])\n\nprint(\"Total number of images we have\",total_images)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:22.958343Z","iopub.status.idle":"2024-05-06T08:48:23.436302Z","shell.execute_reply.started":"2024-05-06T08:48:22.959217Z","shell.execute_reply":"2024-05-06T08:48:23.435200Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Number of images of labels: 0 || Number of images of images: 2017 || Total number of images we have 2017\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 7000x7000 with 0 Axes>"},"metadata":{}}]},{"cell_type":"code","source":"print(plant_names)\nprint(len(plant_names))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:48:23.438066Z","iopub.execute_input":"2024-05-06T08:48:23.438497Z","iopub.status.idle":"2024-05-06T08:48:23.443989Z","shell.execute_reply.started":"2024-05-06T08:48:23.438466Z","shell.execute_reply":"2024-05-06T08:48:23.442408Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"['labels', 'images']\n2\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow\nfrom tensorflow import keras\nfrom keras.models import Sequential, load_model, Model\nfrom keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Dense, Flatten, ZeroPadding2D, BatchNormalization, Activation, Add, Input, Dropout, GlobalAveragePooling2D\nfrom keras.optimizers import SGD\nfrom keras.initializers import glorot_uniform\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#CLAHE Preprocessing\nimport cv2\nimport os\n\n# Dataset directoryD:\\Miniproject\\New Plant Diseases Dataset(Augmented)\\New Plant Diseases Dataset(Augmented)\ndataset_dir = '/kaggle/input/new-plant-diseases-dataset/New Plant Diseases Dataset(Augmented)/New Plant Diseases Dataset(Augmented)/train'\n\n# Output directory\noutput_dir = \"uma_preprocessed\"\n\n# Create output directory if it doesn't exist\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# Define CLAHE parameters (optional)\nclipLimit = 5\n\n# Function to apply CLAHE preprocessing\ndef apply_clahe(image):\n    # Split the image into channels\n    b, g, r = cv2.split(image)\n\n    # Apply CLAHE to each channel\n    clahe = cv2.createCLAHE(clipLimit=clipLimit)\n    clahe_b = clahe.apply(b)\n    clahe_g = clahe.apply(g)\n    clahe_r = clahe.apply(r)\n\n    # Merge the channels back together\n    processed_image = cv2.merge([clahe_b, clahe_g, clahe_r])\n\n    return processed_image\n\n# Iterate through class subfolders\nfor class_name in os.listdir(dataset_dir):\n    if os.path.isdir(os.path.join(dataset_dir, class_name)):\n        # Create output subfolder for this class\n        output_class_dir = os.path.join(output_dir, class_name)\n        if not os.path.exists(output_class_dir):\n            os.makedirs(output_class_dir)\n\n        # Iterate through images in the class subfolder\n        for filename in os.listdir(os.path.join(dataset_dir, class_name)):\n            image_path = os.path.join(dataset_dir, class_name, filename)\n            image = cv2.imread(image_path)\n\n            # Preprocess the image (optional)\n            if apply_clahe:\n                processed_image = apply_clahe(image)\n            else:\n                processed_image = image\n\n            # Save the preprocessed image\n            output_path = os.path.join(output_class_dir, filename)\n            cv2.imwrite(output_path, processed_image)\n\nprint(\"Preprocessing complete.\")","metadata":{"execution":{"iopub.status.busy":"2024-05-06T08:49:14.985924Z","iopub.execute_input":"2024-05-06T08:49:14.986915Z","iopub.status.idle":"2024-05-06T09:05:58.125051Z","shell.execute_reply.started":"2024-05-06T08:49:14.986880Z","shell.execute_reply":"2024-05-06T09:05:58.123563Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Preprocessing complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"!conda install -y gdown ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom sklearn.model_selection import train_test_split\n\ndataset_path = '/kaggle/working/uma_preprocessed'\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Load the entire dataset\nfull_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(full_dataset))  # 80% for training\nval_size = len(full_dataset) - train_size  # 20% for validation\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\n# Create data loaders for the training and validation sets\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# --- Imports --- #\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# --- Channel Attention (CA) Layer --- #\nclass CALayer(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(CALayer, self).__init__()\n        # global average pooling: feature --> point\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        # feature channel downscale and upscale --> channel weight\n        self.conv_du = nn.Sequential(\n                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv_du(y)\n        return x * y \n\nclass PCFAN(nn.Module):\n    def __init__(self):\n        super(PCFAN, self).__init__()\n        self.layer1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1)\n            )\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 32, kernel_size=3, padding=1)\n            )\n        self.layer5 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n        self.layer6 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1)\n            )\n        self.layer7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1)\n            )\n        self.layer9 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n        self.layer10 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1)\n            )\n        self.layer11 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1)\n            )\n        self.Attention_1 = CALayer(channel=32)\n        self.Attention_2 = CALayer(channel=64)\n        self.Attention_3 = CALayer(channel=128)\n        self.Attention_4 = CALayer(channel=192)\n        self.final = nn.Conv2d(224,3,kernel_size=3,padding=1)\n        self.tanh = nn.Tanh()\n\n    def three_scale_feature_extraction_phase(self, picture):\n        B,C,H,W = picture.size()\n        picture0 = picture\n        picture = self.layer1(picture)\n        picture = self.layer2(picture) + picture\n        map1 = self.layer3(picture) + picture     \n\n        map2 = self.layer5(map1)\n        map2 = self.layer6(map2) + map2\n        map2 = self.layer7(map2) + map2 \n        map2 = nn.UpsamplingBilinear2d((H//2,W//2))(map2)\n\n        map3 = self.layer9(map2)    \n        map3 = self.layer10(map3) + map3\n        map3 = self.layer11(map3) + map3 \n\n        map1 = self.Attention_1(map1)\n        map2 = self.Attention_2(map2)\n        map3 = self.Attention_3(map3)\n        map3 = nn.UpsamplingBilinear2d((H//2,W//2))(map3)\n        map23 = torch.cat((map2,map3),1)\n        map23 = self.Attention_4(map23)\n        map23 = nn.UpsamplingBilinear2d((H,W))(map23)\n\n        map123 = torch.cat((map23,map1),1)\n        out = self.tanh(self.final(map123))\n        out = torch.add(picture0,out)\n        return out\n\n    def extract_features(self, picture):\n        features = self.three_scale_feature_extraction_phase(picture)\n        return features\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T17:04:08.679650Z","iopub.execute_input":"2024-03-25T17:04:08.680243Z","iopub.status.idle":"2024-03-25T17:04:08.709136Z","shell.execute_reply.started":"2024-03-25T17:04:08.680214Z","shell.execute_reply":"2024-03-25T17:04:08.707819Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\n\n# Instantiate the model\nmodel = PCFAN()\n\n# Define the directory for the extracted features\nextracted_features_dir = '/kaggle/working/extracted_features'\n\n# Loop over the dataset\nfor i, data in enumerate(train_loader, 0):\n    # Get the inputs; data is a list of [inputs, labels]\n    inputs, labels = data\n\n    # Extract feature maps\n    feature_maps = model.extract_features(inputs)\n\n    # Get the class name from the label\n    class_name = train_loader.dataset.dataset.classes[labels[0].item()]\n\n    # Create a directory for this class, if it doesn't exist already\n    class_dir = os.path.join(extracted_features_dir, class_name)\n    os.makedirs(class_dir, exist_ok=True)\n\n    # Save the feature maps to a pickle file in this directory\n    with open(os.path.join(class_dir, f'feature_maps_{i}.pkl'), 'wb') as f:\n        pickle.dump(feature_maps, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T16:49:11.813674Z","iopub.status.idle":"2024-03-25T16:49:11.814678Z","shell.execute_reply.started":"2024-03-25T16:49:11.814457Z","shell.execute_reply":"2024-03-25T16:49:11.814481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nclass SingleFolderDataset(Dataset):\n    def __init__(self, folder, transform=None):\n        self.transform = transform\n        self.image_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.png') or f.endswith('.jpg') or f.endswith('.JPG')]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = Image.open(img_path)\n        if self.transform:\n            img = self.transform(img)\n        return img\n\ndataset_path = '/kaggle/working/uma_preprocessed/Peach___healthy'\n\n# Define a transform to normalize the data\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Load the dataset\ndataset = SingleFolderDataset(dataset_path, transform=transform)\n\n# Create a data loader for the dataset\ndata_loader = DataLoader(dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-25T18:55:45.202160Z","iopub.execute_input":"2024-03-25T18:55:45.202684Z","iopub.status.idle":"2024-03-25T18:55:45.242988Z","shell.execute_reply.started":"2024-03-25T18:55:45.202638Z","shell.execute_reply":"2024-03-25T18:55:45.242006Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\n\n# Instantiate the model\nmodel = PCFAN()\n\n# Define the directory for the extracted features\nextracted_features_dir = '/kaggle/working/extracted_features'\n\n# Create the directory if it does not exist\nos.makedirs(extracted_features_dir, exist_ok=True)\n\n# Loop over the dataset\nfor i, inputs in enumerate(data_loader, 0):\n    # Extract feature maps\n    feature_maps = model.extract_features(inputs)\n\n    # Save the feature maps to a pickle file in the directory\n    with open(os.path.join(extracted_features_dir, f'feature_maps_{i}.pkl'), 'wb') as f:\n        pickle.dump(feature_maps, f)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-25T18:55:45.254753Z","iopub.execute_input":"2024-03-25T18:55:45.255830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r file.zip /kaggle/working/extracted_features","metadata":{"execution":{"iopub.status.busy":"2024-03-25T19:25:47.531356Z","iopub.execute_input":"2024-03-25T19:25:47.531847Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/extracted_features/ (stored 0%)\n  adding: kaggle/working/extracted_features/feature_maps_6.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_17.pkl (deflated 11%)\n  adding: kaggle/working/extracted_features/feature_maps_18.pkl (deflated 11%)\n  adding: kaggle/working/extracted_features/feature_maps_7.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_1.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_23.pkl (deflated 10%)\n  adding: kaggle/working/extracted_features/feature_maps_15.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_26.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_16.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_22.pkl (deflated 14%)\n  adding: kaggle/working/extracted_features/feature_maps_19.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_5.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_3.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_24.pkl (deflated 11%)\n  adding: kaggle/working/extracted_features/feature_maps_0.pkl (deflated 11%)\n  adding: kaggle/working/extracted_features/feature_maps_14.pkl (deflated 8%)\n  adding: kaggle/working/extracted_features/feature_maps_2.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_4.pkl (deflated 13%)\n  adding: kaggle/working/extracted_features/feature_maps_8.pkl (deflated 14%)\n  adding: kaggle/working/extracted_features/feature_maps_13.pkl (deflated 11%)\n  adding: kaggle/working/extracted_features/feature_maps_10.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_21.pkl (deflated 10%)\n  adding: kaggle/working/extracted_features/feature_maps_20.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_11.pkl (deflated 12%)\n  adding: kaggle/working/extracted_features/feature_maps_25.pkl","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'file.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/extracted_features","metadata":{"execution":{"iopub.status.busy":"2024-03-25T18:55:07.199826Z","iopub.execute_input":"2024-03-25T18:55:07.200970Z","iopub.status.idle":"2024-03-25T18:55:08.612552Z","shell.execute_reply.started":"2024-03-25T18:55:07.200906Z","shell.execute_reply":"2024-03-25T18:55:08.611162Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/file.zip","metadata":{"execution":{"iopub.status.busy":"2024-03-25T18:55:08.615555Z","iopub.execute_input":"2024-03-25T18:55:08.616039Z","iopub.status.idle":"2024-03-25T18:55:09.921784Z","shell.execute_reply.started":"2024-03-25T18:55:08.615990Z","shell.execute_reply":"2024-03-25T18:55:09.920086Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}